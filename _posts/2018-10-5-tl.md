---
layout: post
title: Notes on Transfer Learning
permalink: /blog/:title
---

Under the concept of Learning to Learn, Transfer Learning was among the first breakthroughs in the field. Discussed in NIPS-95 workshop, Transfer Learning operates outside of the common machine learning assumption of having to draw samples from the same feature space and distribution. It allows the domains, tasks, and distributions used in training and testing to be different. 


Theoretically, a domain $$\mathcal{D}$$ composes of feature space $$\mathcal{X}$$ and marginal distribution $$P(\mathcal{X})$$. For learning task $$\mathcal{T}_s$$ in source domain $$\mathcal{D}_s$$, Transfer Learning improves the learning of the target predictive function $$f_T(.)$$ in $$\mathcal{D}_T$$ using knowledge in $$\mathcal{D}_s$$ and $$\mathcal{T}_s$$.


From here, Transfer Learning can be categorized mainly into three archetypes, inductive, transductive, and unsupervised.

Inductive Transfer Learning is common in the real world, and incredibly useful in NAS. The model assumes that $$\mathcal{T}_{target}$$ is different from $$\mathcal{T}_{source}$$. This knowledge, taken as the induced information through learning procedure from source domain, can be applied to the target domain with some influence control . 
      
Transductive Transfer Learning assumes the same $$\mathcal{T}$$ but different $$\mathcal{D}$$. Most real world application involves the case where source domain contains large labeled data. We want to build an effective learning algorithm applicable to the target domain without more manual labeling.  

Unsupervised Transfer Learning is quite similar to inductive Transfer Learning. Nevertheless, it does not need label and emphasizes solving unsupervised tasks in the target domain, including density estimation.  
