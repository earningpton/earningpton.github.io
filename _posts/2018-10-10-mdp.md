---
layout: post
title: Notes on Near-Optimal Time and Sample Complexities for for Solving DMDP
permalink: /blog/:title
---

These are notes I've compiled over the course of reading [this paper](https://arxiv.org/pdf/1806.01492.pdf), which was published recently by Sidford et al.  I came across this paper at the suggestion of one of the authors in the paper, who is a [professor](http://www.princeton.edu/~mengdiw/) in my department.  I want crystalize my thoughts of the paper to test and record my understanding, but maybe this will be useful to others trying to get started in RL research.

## Intro: the typical framework studied in reinforcement learning

Let $$\cal{S}$$ be the set of all states, let $$\cal{A}$$ be the set of all actions that can be taken from any state.  

Then let $$ v $$ be a the value vector.  $$ v_{i} $$ represents the expected payoff if we enter state $$ i $$.  Let $$ P $$ be the state-action-state transition matrix, which is essentially a tensor.  Think of it as a $$ \|\cal{S}\|\times\|\cal{A}\| $$ matrix, with a vector of length $$ \|\cal{S}\| $$ in every entry. We assign in $$ P_{s,a,s'} $$ the probability that we go to state $$ s' $$ if we take action $$ a $$ from state $$ s $$.

Let $$ r_{s,a} $$ be the reward for taking action $$ a $$ at state $$ s $$, and assume each reward is between 0 and 1.  Now note that $$Pv$$ is a $$ \|\cal{S}\|\times\|\cal{A}\| $$ matrix, where each entry is $$ P_{s,a}^{T}v $$.  If you think about it, $$ P_{s,a}^{T}v $$ represents the expected payoff after taking action $$ a $$ from $$ s $$ (just write out the dot product and it becomes clear).

Now, what we want to do is to come up with a policy $$ \pi $$, which is a function that takes in a state and spits out the action, defined over all states.  The best policy assigns actions to states in such a way that maximizes the reward over the entire process (in the example above, the entire chess game).  We call this policy $$ \pi^{*} $$.  

Call $$ T(v)_{s} = max_{a \in \cal{A}} {r_{s,a} + \gamma P_{s,a}^{T}v} $$ the value operator on a given value vector.  $$\gamma \in (0,1)$$ here represents the discount factor, or amount we discount future value.  Note that the function returns another vector.  The interpretation here is that the value of a state is equal to the reward from taking that action at the current state plus the expected payoff after taking that state (Note $$P_{s,a}^{T}v$$, as stated earlier, is the expected payoff after taking action a from state s) times the discount factor, which decreases the payoff by the factor specified.  

We define the value of the optimal policy as $$ T(v^{*}) = v^{*} $$.  I am not sure what the interpretation is here; I'll update this after I talk to Professor Wang about it.

Call $$T_{\pi}(v) = r_{s,\pi(s)} + \gamma P_{s,\pi(s)}^{T}v $$ as the value operator associated with $$\pi$$.  Note that this definition is the same as the one above but no maximum is taken, as $$a$$ is determined.  The value associated with $$\pi$$ is defined as $$v^{\pi}$$ where $$T_{\pi}(v^{\pi}) = v^{\pi}$$

A value $$v$$ is $$\epsilon$$-optimal if $$\|v^{*} - v\|_{\infty} \leq \epsilon$$.


