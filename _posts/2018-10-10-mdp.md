---
layout: post
title: Notes on Sample/Time Complexities for Solving DMDP with a Generative Model
permalink: /blog/:title
---

These are notes I've compiled over the course of reading [this paper](https://arxiv.org/pdf/1806.01492.pdf), which was published recently by Sidford et al.  I came across this paper at the suggestion of one of the authors in the paper, who is a [professor](http://www.princeton.edu/~mengdiw/) in my department.  I want crystalize my thoughts of the paper to test and record my understanding, but maybe this will be useful to others trying to get started in RL research.

## Intro: the typical framework studied in reinforcement learning
To motivate the the topic, think of the problem of designing a robot to play a game of chess.  When it is the robot's turn, it can move its pieces in many different ways.  Depending on which piece it moved and where it moved it, the different states of the chess board when it comes back to the robot's turn has different probabilities of occurring, the randomness arising because we don't know how the opponent will play.  Say we had a measure for the quality of the position of the chess bot.  Then the bot should try to maximize that measure.  This is a classical example of the kinds of problems RL tries to solve.  

On a very high level, the goal of reinforcement learning is to maximize the reward (difference in the measure of quality in the example above) that an agent accrues as the result of taking actions in an environment that interacts with it.  The "states" (the states of the board when it is the bot's turn above) encapsulate all the different scenarios that an agent can find themselves in as a result of taking actions in the environment, and the "actions" (the different ways the bot can move its pieces) represent all the things that the agent can do.  At each step, the agent accrues a certain amount of reward by taking certain actions at certain states.  

Some of the definitions and notions here can be made more general, but I tried to make them more concrete and give them the typical interpretation seen in literature to make it easier to understand for beginners.

Let $$\cal{S}$$ be the set of all states, let $$\cal{A}$$ be the set of all actions that can be taken from any state.  

Then let $$ v $$ be a the value vector.  $$ v_{i} $$ represents the expected payoff if we enter state $$ i $$.  Let $$ P $$ be the state-action-state transition matrix, which is essentially a tensor.  Think of it as a $$ \|\cal{S}\|\times\|\cal{A}\| $$ matrix, with a vector of length $$ |\cal{S}| $$ in every entry. We assign in $$ P_{s,a,s'} $$ the probability that we go to state $$ s' $$ if we take action $$ a $$ from state $$ s $$.

Let $$ r_{s,a} $$ be the reward for taking action $$ a $$ at state $$ s $$, and assume each reward is between 0 and 1.  Now note that $$Pv$$ is a $$ |\cal{S}|\times|\cal{A}| $$ matrix, where each entry is $$ P_{s,a}^{T}v $$.  If you think about it, $$ P_{s,a}^{T}v $$ represents the expected payoff from taking action $$ a $$ from $$ s $$ (just write out the dot product and it becomes clear).

Now, what we want to do is to come up with a policy $$ \pi $$, which is a function that takes in a state and spits out the action, defined over all states.  The best policy assigns actions to states in such a way that maximizes the reward over the entire process (in the example above, the entire chess game).  We call this policy $$ \pi^{*} $$.  


