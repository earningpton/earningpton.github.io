---
layout: post
title: Notes on Neural Architecture Search
permalink: /blog/:title
---

In less than three years, Neural Architect Search (NAS) has continued to outperform manually designed architectures while being greatly improved upon. Here, I have compiled details about NAS into easily readable format

### Automating Deep Reinforcement Learning Deep Reinforcement Learning

From SIFT(Lowe, 1999) to GoogleNet (Szegedy et al., 2015), there has been a subtle but constant paradigm shift from feature designing to architecture designing. Fundamentally, the concept of NAS and Transfer Learning (TL) or knowledge transfer both evolved from a subsection of the Learning to Learn narrative. The idea of Learning to Learn focused on the need for lifelong machine learning methods that retain and reuse previously learned knowledge given a task, a training experience and a performance measure. NAS originally implements the idea of Learning to Learn in the form of the gradient descent updates sharing and only apply Transfer Learning to the final results. Only recently has NAS utilized Transfer Learning techniques directly to find best performing learning algorithms through deep reinforcement learning. Ultimately, one primary goal of the Automatic Machine Learning (AutoML) field is to find best performing learning algorithms with minimal human intervention possible \cite{NASNET, PNAS, ENAS, NASRL, TLNAM}.

#### Neural Architecture Search

NAS aims to generate models for performance optimization in a given task. Through deep reinforcement learning, NAS framework consists of a controller model and child models. Controller maximizes performance depending on tasks by constructing network configurations. The architecture of the child models is then defined by these configurations. Child models are then trained under the current task on each iteration and their evaluations on the validation set are calculated as rewards to update the controller through a policy gradient algorithm .

Classically, the model descriptions of neural networks or the controller's architecture are constructed as recurrent neural networks (RNN). The controller RNN samples a sequence of discrete actions, generally a simple convolution network. Each action then specifies a design choice such as filter height, filter width, stride height, stride width and then fed as input into RNN. The benefits of RNN allows decision sequence length control and keeping design choices record for dependencies learning. Gradient method is generally used to update the parameters $\theta_c$ of the controller RNN, which maximize expected validation accuracy of the proposed architectures at convergence. In general, the controller is trained through reinforcement learning to maximize expected reward $$\mathit{J}(\theta_c)$$ = $$\mathit{E}_{\mathit{P}(a_{1:T};\theta_c)}[\mathit{R}]$$ where accuracy R is the child network accuracy on held-out data at convergence. Because R is not differentiable, we then apply the REINFORCE rule from Williams (1992) as a  policy gradient method to iteratively update $$\theta_c$$ instead.

#### Neural AutoML: ENAS, PNAS, NASNet, and MNASNet

Regarding computational resources, neural Auto Machine Learning method (NAS-type) is among the more expensive of many other AutoML methods that range from random search to genetic algorithms. The performance of neural AutoML, however, outperforms human architectured models without domain specific knowledge, making this an incredibly rich problem to take on.

Currently, Efficient Neural Architecture Search (ENAS) and Progressive Neural Architecture Search (PNAS) are the contending state-of-the-art neural AutoML structures. They both significantly outperforms the NASNet Zoph et al. model in terms of GPU-hours and still show strong empirical performances on par with NASNet, which has state-of-the-art accuracy itself . Below, we briefly summarize the four NAS types and how we can incorporate the advantages of each model into our transferable NAS optimizer.

  NASNet: NASNet search space allows for transferability, improving the speed of traditional NAS by searching over architectural building block on a small dataset before transferring the learned knowledge to a larger dataset    

  MNASNet: Mobile NASNet tunes over latency in addition to accuracy, taking into account the mobile device's constraints. The model provides 2.4$$\times$$NASNet efficiency while constantly outperforming state-of-the-art mobile convolutional neural networks (CNN). We mention MNASNet due to its potential integration with ENAS for our implementation case in a mobile device where latency and low calculating power has to be balanced with accuracy.  

  PNAS: PNAS utilizes a sequential model-based optimization (SMBO) strategy} to learn CNN. It searches for structures in order of increasing complexity while learning a surrogate model to guide the search. The model stacks layers of cell each containing $$\mathit{B}$$ blocks, each of which is a combination operator. The model is then highly transferable between different datasets, such as from CIFAR-10 to ImageNet. For our purpose, however, much more tinkering has to be done.  

  ENAS: ENAS is both fast and computationally cheap. ENAS creates a huge computational graph where each subgraph is a neural network, forcing all architectures to share parameters and all child models to share weights. The parameter sharing aspect is highly desirable for Transfer Learning. With some optimization, the ENAS-based transfer model can do Transfer Learning with even more convenience and efficiency than traditional Transfer Learning.  
